{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Caio-Giacomelli/FIAP-3IADT-TC-04/blob/main/FIAP_3IADT_TC04.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FIAP - IA Para Devs - Turma 3IADT 2024 - Trabalho 4\n",
        "### Caio Henrique Giacomelli         - RM 358131\n",
        "### Rafael Pereira Alonso            - RM 358127\n",
        "### Wagner Dominike Eugênio de Mello - RM 358565"
      ],
      "metadata": {
        "id": "DYuDauS-TiY6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Link do Github: https://github.com/Caio-Giacomelli/FIAP-3IADT-TC-04\n",
        "### Link do Youtube: https://www.youtube.com/watch?v=C2iriFcsIgw"
      ],
      "metadata": {
        "id": "5t6GAi6sTsK4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Resumo\n",
        "\n",
        "Neste Tech Challenge, realizamos o desafio de implementar reconhecimento facial, análise de expressões emocionais, detecção de atividades em tempo real e geração de resumo destas atividades. Em conjunto, definimos uma anomalia e à detectamos em vídeo.\n",
        "\n",
        "\n",
        "O reconhecimento facial foi realizado de duas formas. Utilizamos haarcascades como classificador da biblioteca de cv2 em conjunto com Deep Face para identificar o rosto e mapear as emoções associadas ao rosto detectado, já também solucionando o segundo desafio, de análise de expressões emocionais. O método detect_face_expressions é o responsável por esta lógica. Também utilizamos o MediaPipe para identificação de rosto, porém utilizamos seus landmarks para definir anomalias, escolhendo a distância entre o nariz e a boca superiores ou inferiores à um limiar, para ser identificada como anomalia.\n",
        "\n",
        "A geração de resumo em tempo real foi realizada utilizando BlipProcessor e BlipForConditionalGeneration, da biblioteca de transformers.E para finalizar, resumimos as Captions e Emoções utilizando o Summarizer\n",
        "\n",
        "**Total de Frames analisados:** 3326\n",
        "\n",
        "**Total de Anomalias encontradas:** 35\n",
        "\n",
        "**Tempo total de processamento:** 32 minutos\n",
        "\n",
        "**Principais emoções encontradas:** Neutra, medo, felicidade, raiva, tristeza, surpresa\n"
      ],
      "metadata": {
        "id": "SP41166yXKpX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importação de bibliotecas e Conexão com Drive"
      ],
      "metadata": {
        "id": "10vu00NrTwZf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mtf0Hc-9BAur"
      },
      "outputs": [],
      "source": [
        "!pip install opencv-python mediapipe tqdm deepface transformers --quiet --use-deprecated=legacy-resolver"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ESNlcTFbBJoD"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "from tqdm import tqdm\n",
        "import mediapipe as mp\n",
        "from deepface import DeepFace\n",
        "from transformers import BlipProcessor, BlipForConditionalGeneration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q_UPscVvBk8T"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Métodos para Detecção de Poses, expressões faciais e anomalias"
      ],
      "metadata": {
        "id": "2105GtPcT1Le"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ldFILg1_CCpb"
      },
      "outputs": [],
      "source": [
        "def detect_face_expressions(frame, face_cascade):\n",
        "  gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "  faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=10, minSize=(30, 30))\n",
        "\n",
        "  emotions_in_frame = []\n",
        "  for (x, y, w, h) in faces:\n",
        "    face_roi = frame[y:y+h, x:x+w]\n",
        "    try:\n",
        "      analysis = DeepFace.analyze(face_roi, actions=['emotion'], silent=True, enforce_detection=False)\n",
        "      emotion = analysis[0]['dominant_emotion']\n",
        "      cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
        "      cv2.putText(frame, emotion, (x, y-10), cv2.FONT_HERSHEY_DUPLEX, 0.9, (0, 255, 0), 2)\n",
        "      emotions_in_frame.append(emotion)\n",
        "    except ValueError as e:\n",
        "      print(f\"Error analyzing face: {e}\")\n",
        "  return emotions_in_frame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WTjjFOnfCIgM"
      },
      "outputs": [],
      "source": [
        "def describe_frames(frames, processor, model, frame_width, frame_height):\n",
        "  inputs = processor(images=frames, return_tensors=\"pt\")\n",
        "  captions = model.generate(**inputs)\n",
        "  return [processor.decode(caption, skip_special_tokens=True) for caption in captions]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def detect_face_anomalies(frame, mp_face_mesh, face_mesh, mp_drawing):\n",
        "  rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "  results = face_mesh.process(rgb_frame)\n",
        "\n",
        "  anomaly_count = 0\n",
        "\n",
        "  if results.multi_face_landmarks:\n",
        "    for face_landmarks in results.multi_face_landmarks:\n",
        "      # Obter as coordenadas do bounding box do rosto a partir dos landmarks\n",
        "      x_coords = [landmark.x for landmark in face_landmarks.landmark]\n",
        "      y_coords = [landmark.y for landmark in face_landmarks.landmark]\n",
        "      x_min, x_max = min(x_coords), max(x_coords)\n",
        "      y_min, y_max = min(y_coords), max(y_coords)\n",
        "\n",
        "      frame_height, frame_width, _ = frame.shape\n",
        "      face_width = (x_max - x_min) * frame_width\n",
        "\n",
        "      # Anomalia: verificar a distância entre a boca e o nariz\n",
        "      # Acesse os landmarks usando os índices numéricos\n",
        "      nose_tip = face_landmarks.landmark[4]        # Ponto da ponta do nariz\n",
        "      upper_lip = face_landmarks.landmark[13]     # Ponto médio do lábio superior\n",
        "\n",
        "      # Converter coordenadas normalizadas para coordenadas de pixel\n",
        "      nose_tip_coords = (int(nose_tip.x * frame_width), int(nose_tip.y * frame_height))\n",
        "      upper_lip_coords = (int(upper_lip.x * frame_width), int(upper_lip.y * frame_height))\n",
        "\n",
        "      # Calcular a distância entre a ponta do nariz e o ponto médio do lábio superior\n",
        "      distance_nose_lip = math.dist(nose_tip_coords, upper_lip_coords)\n",
        "\n",
        "      # Normalizar a distância pela largura do rosto\n",
        "      normalized_distance = distance_nose_lip / face_width if face_width > 0 else 0\n",
        "\n",
        "      # Definir um limiar para anomalia\n",
        "      anomaly_threshold_min = 0.15\n",
        "      anomaly_threshold_max = 0.293\n",
        "\n",
        "      # print(f\"normalized_distance: \" + str(normalized_distance))\n",
        "      # print(f\"anomaly_threshold: \" + str(anomaly_threshold))\n",
        "\n",
        "      if normalized_distance <= anomaly_threshold_min or normalized_distance > anomaly_threshold_max:\n",
        "        # Marcar a anomalia\n",
        "        mid_point_x = int((nose_tip_coords[0] + upper_lip_coords[0]) / 2)\n",
        "        mid_point_y = int((nose_tip_coords[1] + upper_lip_coords[1]) / 2)\n",
        "        cv2.circle(frame, (mid_point_x, mid_point_y), radius=10, color=(0, 0, 255), thickness=-1)\n",
        "        print(f\"Anomalia na distância do nariz e boca detectada: {normalized_distance:.2f}).\")\n",
        "        anomaly_count += 1\n",
        "\n",
        "      # Desenhar os landmarks faciais\n",
        "      mp_drawing.draw_landmarks(\n",
        "          frame,\n",
        "          face_landmarks,\n",
        "          mp_face_mesh.FACEMESH_CONTOURS, # Ou outra opção de conexão\n",
        "          mp_drawing.DrawingSpec(color=(0, 255, 0), thickness=1, circle_radius=1),\n",
        "          mp_drawing.DrawingSpec(color=(0, 255, 0), thickness=1, circle_radius=1))\n",
        "\n",
        "  return anomaly_count"
      ],
      "metadata": {
        "id": "CDL8U7zy04mY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def detect_poses(frame, mp_pose, pose, mp_drawing):\n",
        "  rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "  results = pose.process(rgb_frame)\n",
        "\n",
        "  if results.pose_landmarks:\n",
        "      mp_drawing.draw_landmarks(frame, results.pose_landmarks, mp_pose.POSE_CONNECTIONS)"
      ],
      "metadata": {
        "id": "G9ImxpVoJYDD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementação do Laço para coleta e processamento dos Frames"
      ],
      "metadata": {
        "id": "IDb_6fJoT7Za"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o2FlGk1dCKeH"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "def process_video(video_path, output_path):\n",
        "  cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "  if not cap.isOpened():\n",
        "    print(\"Erro ao abrir o vídeo.\")\n",
        "    return\n",
        "\n",
        "  width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "  height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "  fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "  total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "  fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "  out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
        "\n",
        "  # frame describer\n",
        "  processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "  model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "\n",
        "  # face recognition\n",
        "  face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
        "\n",
        "  # pose recognition\n",
        "  mp_pose = mp.solutions.pose\n",
        "  pose = mp_pose.Pose()\n",
        "  mp_drawing = mp.solutions.drawing_utils\n",
        "\n",
        "  print(\"Iniciando coleta dos frames...\")\n",
        "  frames = []\n",
        "  for _ in tqdm(range(total_frames), desc=\"Coletando frames\"):\n",
        "    ret, frame = cap.read()\n",
        "\n",
        "    if not ret:\n",
        "      break\n",
        "\n",
        "    frames.append(frame)\n",
        "\n",
        "  print(\"Iniciando processamento...\")\n",
        "  batch_size = fps\n",
        "  total_batches = math.ceil(len(frames) / batch_size)\n",
        "  processed_seconds = []\n",
        "  total_anomaly_count = 0\n",
        "\n",
        "\n",
        "  mp_face_mesh = mp.solutions.face_mesh\n",
        "  face_mesh = mp_face_mesh.FaceMesh(static_image_mode=False, max_num_faces=1, min_detection_confidence=0.5)\n",
        "\n",
        "  for i in range(0, len(frames), batch_size):\n",
        "    batch_frames = frames[i:i + batch_size]\n",
        "    current_batch = (i // batch_size) + 1\n",
        "\n",
        "    # get description for a second of video\n",
        "    print(f\"Gerando descrições de frames...\")\n",
        "    captions = describe_frames(batch_frames[0], processor, model, width, height)\n",
        "    caption_text = captions[0]\n",
        "    emotions_in_fps = []\n",
        "\n",
        "    for frame in tqdm(batch_frames, desc=f\"Processando batch {current_batch}/{total_batches}\"):\n",
        "\n",
        "      # detect face expressions per frame\n",
        "      emotions_in_frame = detect_face_expressions(frame, face_cascade)\n",
        "      emotions_in_fps += emotions_in_frame\n",
        "\n",
        "      # detect anomaly\n",
        "      anomaly_count = detect_face_anomalies(frame, mp_face_mesh, face_mesh, mp_drawing)\n",
        "      total_anomaly_count += anomaly_count\n",
        "\n",
        "      # print description in frame\n",
        "      cv2.rectangle(frame, (0, height), (width, height - 70), (0, 0, 0), -1)\n",
        "      cv2.putText(frame, caption_text, (10, height - 30), cv2.FONT_HERSHEY_DUPLEX, 1, (0, 255, 255), 2)\n",
        "\n",
        "      # save frame in output video\n",
        "      out.write(frame)\n",
        "\n",
        "    processed_seconds.append({ 'caption': caption_text, 'emotions': set(emotions_in_fps) })\n",
        "  print(\"\\nProcessamento concluído!\")\n",
        "\n",
        "  cap.release()\n",
        "  out.release()\n",
        "  cv2.destroyAllWindows()\n",
        "  return { 'total_frames': len(frames), 'results_per_second': processed_seconds, 'anomaly_count': total_anomaly_count }"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Manipulação dos vídeos e inicialização do processamento"
      ],
      "metadata": {
        "id": "xdDh3R0jUDcM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "of0lsO1gDrao"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "\n",
        "folder_path = '/content/drive/MyDrive/FIAP/Trabalho04'\n",
        "input_video_path = os.path.join(folder_path, 'video', 'Unlocking Facial Recognition_ Diverse Activities Analysis.mp4')\n",
        "output_video_path = os.path.join(folder_path, 'output', 'output_video.mp4')\n",
        "\n",
        "if not os.path.exists(input_video_path):\n",
        "  raise FileNotFoundError(f\"O vídeo de entrada não existe em {input_video_path}\")\n",
        "\n",
        "start_time = time.time()\n",
        "processed_frames = process_video(input_video_path, output_video_path)\n",
        "end_time = time.time()\n",
        "print(f\"Tempo de processamento: {(end_time - start_time) // 60} minutos\")\n",
        "print(f\"Total de frames processados: {processed_frames['total_frames']}\")\n",
        "print(f\"Total de Anomalias: {processed_frames['anomaly_count']}\")\n",
        "\n",
        "if not os.path.exists(output_video_path):\n",
        "  raise FileNotFoundError(f\"Erro ao gerar video de saída em {output_video_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4TuT-qwwEevI"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "files.download(output_video_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summarização das Captions e Emoções do vídeo"
      ],
      "metadata": {
        "id": "uJNXcZUVUPAS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "processed_frames['results_per_second']"
      ],
      "metadata": {
        "id": "FP43M2xYMWEJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "summarizer = pipeline(\"summarization\")\n",
        "\n",
        "# Extrai captions e emoções únicas\n",
        "all_captions = [d['caption'] for d in processed_frames['results_per_second']]\n",
        "all_emotions = []\n",
        "for d in processed_frames['results_per_second']:\n",
        "    emotions = d.get('emotions')\n",
        "    if emotions:\n",
        "        all_emotions.extend(list(emotions))\n",
        "\n",
        "unique_emotions = list(set(all_emotions))\n",
        "unique_captions = list(set(all_captions))\n",
        "\n",
        "# Sumarização das Captions\n",
        "caption_input_text = f\"Captions:\\n{', '.join(unique_captions)}\"\n",
        "caption_summary = summarizer(caption_input_text, max_length=100, min_length=30, do_sample=True)\n",
        "print(\"Sumário das Captions:\")\n",
        "print(caption_summary[0]['summary_text'])\n",
        "\n",
        "print(\"-\" * 20)\n",
        "\n",
        "# Sumarização das Emoções\n",
        "emotion_input_text = f\"Emotions:\\n{', '.join(unique_emotions)}\"\n",
        "emotion_summary = summarizer(emotion_input_text, max_length=100, min_length=30, do_sample=True)\n",
        "print(\"Sumário das Emoções:\")\n",
        "print(emotion_summary[0]['summary_text'])"
      ],
      "metadata": {
        "id": "KaBMFneTl6jg"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "10vu00NrTwZf",
        "2105GtPcT1Le",
        "IDb_6fJoT7Za",
        "uJNXcZUVUPAS"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
